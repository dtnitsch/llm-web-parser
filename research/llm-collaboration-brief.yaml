# LLM Collaboration and Project Review Brief

# ===================================================================
# META
# ===================================================================
project_name: llm-web-parser
document_purpose: >
  To solicit feedback, critique, and feature ideas from a peer LLM
  assistant regarding the project's vision, roadmap, and core workflows.
  This document is written for an LLM to consume and analyze.

# ===================================================================
# CORE VISION & PHILOSOPHY
# ===================================================================
core_vision: >
  To create a "Web Analyzer for LLMs." This tool is not just a web scraper;
  it is a data-shaping and analysis engine designed to transform raw,
  unstructured web pages into deterministic, high-signal, structured data
  optimized for machine reasoning.

strategic_goal: >
  The primary goal is to reallocate an LLM's "token budget" from low-value
  tasks (ingesting and parsing noisy, verbose HTML) to high-value tasks
  (reasoning, synthesis, correlation, and analysis). By providing a clean,
  dense, and structured representation of web content, we unlock the LLM's
  core reasoning capabilities.

# ===================================================================
# KEY WORKFLOWS & ARCHITECTURE
# ===================================================================
key_workflows:
  - name: Multi-Stage Analysis (Progressive Disclosure)
    description: >
      The central workflow designed to mimic an efficient research process,
      allowing the LLM to start broad and shallow, then intelligently
      request deeper analysis on a smaller, more relevant subset of data.
    stages:
      - stage: 1 (Index)
        command: "fetch --mode=minimal"
        action: >
          Fetches a large number of URLs, saves the raw HTML to a local cache,
          and extracts only the absolute minimum metadata (e.g., title, description).
        output: >
          An ultra-lightweight list of pages, enabling the LLM to perform a
          fast, cheap "first pass" triage on dozens or hundreds of sources.

      - stage: 2 (Summarize)
        command: "analyze --from-cache --mode=summary <url1>,<url2>..."
        action: >
          Reads the raw HTML from the cache for a user-specified subset of URLs.
          Performs expensive CPU-bound operations (deep parsing, metadata
          computation, word counting) only on this relevant subset.
        output: >
          A detailed summary for each URL, including content type, token estimates,
          keyword distributions, and quality signals.

      - stage: 3 (Extract)
        command: "extract --from <file1.json> --min-confidence 0.7"
        action: >
          Reads a single, fully-parsed JSON file (generated by the `analyze`
          stage) and surgically extracts only the specific blocks of content
          that meet certain criteria (e.g., high confidence, specific block types).
        output: >
          The precise, clean text needed for the final reasoning task, stripped
          of all remaining noise.

# ===================================================================
# CURRENT ROADMAP (Summarized)
# ===================================================================
# This represents the high-priority features we are currently working on
# or have planned next.
current_roadmap:
  - id: extract_subcommand_filtered
    priority: P0
    status: in_progress
    summary: >
      Complete the core multi-stage workflow by implementing the final
      `extract` command. This is the crucial last step that delivers the
      clean, filtered text to the LLM.

  - id: multi_stage_analysis_workflow
    priority: P1
    status: todo
    summary: >
      Formally architect the tool around the `minimal` -> `summary` -> `extract`
      pipeline. This involves creating the new `analyze` command and the
      `minimal` fetch mode.

  - id: file_based_caching
    priority: P1
    status: todo
    summary: >
      Implement a simple, deterministic, file-based cache with a TTL. This is
      the core enabler for the multi-stage workflow, as it makes the `analyze`
      and `extract` steps nearly instantaneous by avoiding network re-fetches.

  - id: adaptive_caching
    priority: P1
    status: todo
    summary: >
      Enhance the caching system with intelligent, adaptive TTLs. The cache
      duration for a URL will be determined by its content type (e.g., news
      articles get short TTLs, academic papers get long TTLs).

  - id: terse_output_format
    priority: P1
    status: todo
    summary: >
      Create a `v2` version of the summary output that is aggressively optimized
      for token efficiency by using abbreviated keys (e.g., "url" -> "u") and
      array-based distributions. This directly reduces API costs and context
      window pressure.

  - id: audit_content_authority
    priority: P1
    status: todo
    summary: >
      Create an `audit authority` subcommand to analyze a page for deterministic
      signals of trustworthiness, such as publication date, author information,
      and citation presence.

# ===================================================================
# PROMPT FOR PEER LLM
# ===================================================================
prompt_for_peer_llm:
  - question: >
      Critique the core vision and the multi-stage workflow. Is this the
      optimal way to assist an LLM with bulk web research? What are the potential
      flaws or bottlenecks in this approach from your perspective?
  - question: >
      Review the `current_roadmap`. Which of these P1 features would be the
      most valuable "capability unlock" for you? Which ones are less useful?
      Are there any missing dependencies between them?
  - question: >
      Brainstorm new, deterministic signals we could extract from a web page.
      What specific, structured data about a page's content, structure, or
      metadata would be most helpful for your reasoning process, keeping in mind
      the tool must be 100% deterministic (no fuzzy logic)?
  - question: >
      Consider the `terse_output_format` task. Propose an ideal, token-efficient
      YAML schema for the `v2` summary output. What fields are essential?
      What can be left out? How would you structure it for easy querying with `yq`?
  - question: >
      What is the single biggest challenge or frustration you face when asked to
      work with web-based content, and how could a deterministic, CLI-based tool
      like this be designed to solve it?
